---
title: Transformer
date: 2025-09-22 15:17:45
permalink: /pages/222b76/
categories:
  - 强大的AI
  - 深度学习
tags:
  - Transformer
author: 
  name: Mr.Kusch
  link: https://github.com/kuschzzp
---

# Transformer

[论文PDF](https://arxiv.org/pdf/1706.03762v7)

Transformer 可以分为两大部分：**Encoder（编码器）** 和 **Decoder（解码器）**。
Encoder 负责“理解输入的内容”，Decoder 负责“根据理解来生成输出”。

## 一、Encoder（编码器）

输入：一句话（序列化后的词，比如 \["Today", "小明", "吃", "苹果"]）

流程：

1. **Embedding + Positional Encoding（词向量 + 位置编码）**

    * 把每个词变成一个向量（embedding）根据词向量的定义，语义相似的两个词对应的词向量的点积应该大于0，而语义不相似的词向量点积应该小于0。
    * 给每个词加上“位置信息”（因为 Transformer 没有循环结构，不知道词的顺序，需要位置编码告诉它）。
    * 得到输入向量 **X₀**。
      👉 类比：就像把“词”翻译成数字语言，再附带“第几个词”的标签。

2. **Multi-Head Self-Attention（多头自注意力）**

    * 为 X₀ 生成 Q（问题）、K（索引）、V（答案内容）。
    * 每个词都会去“问”其他词：“你和我有关系吗？”
    * 通过点积算相关性，然后加权合成一个新的表示。
    * 得到融合上下文的新向量 **X₁**。
      👉 类比：一个班级里，每个同学都可以向其他同学“打听”信息，然后综合大家的答案更新自己。
    > 在我们的实际应用中，我们往往只需要计算 Query 和 Key 之间的注意力结果，很少存在额外的真值 Value。也就是说，我们其实只需要拟合两个文本序列。在经典的 注意力机制中，Q 往往来自于一个序列，K 与 V 来自于另一个序列，都通过参数矩阵计算得到，从而可以拟合这两个序列之间的关系。例如在 Transformer 的 Decoder 结构中，Q 来自于 Decoder 的输入，K 与 V 来自于 Encoder 的输出，从而拟合了编码信息与历史信息之间的关系，便于综合这两种信息实现未来的预测。
   
3. **Feed Forward Network（前馈神经网络）**

    * 每个位置单独经过一个两层的小网络：

        * 先映射到高维（增加表达能力），再压回原维度。
        > FFN 第一层（W1 可训练矩阵） → 投射到更高维 → 给向量更多自由度去组合和表达上下文特征
        > ReLU + 第二层线性（W2） → 投回原维度 → 向量表示更丰富、更抽象，但维度还是原来的
    * 得到更丰富的表达 **X₂**。
      👉 类比：给每个学生配一个“脑补小引擎”，让他们自己加工学到的信息。

4. **Residual + Layer Normalization（残差连接 + 层归一化）**

    * 把输入直接加回输出（残差），防止信息丢失和梯度消失。
    * LayerNorm 保持数值稳定。
    * 得到最终的 Encoder 层输出 **X\_encoder**。
      👉 类比：学生更新完知识后，还保留了之前的记忆，不会忘记。

> Encoder 会堆叠很多层（常见 6 层以上），让句子表示越来越抽象、越来越有信息。

---

## 二、Decoder（解码器）

输入：目标序列（比如正在生成的句子前几个词） + Encoder 输出。

流程：

1. **Masked Multi-Head Self-Attention（带遮挡的自注意力）**

    * 和 Encoder 自注意力类似，但 **不能看到未来的词**（防止作弊）。
    * 得到向量 **Y₁**。
      👉 类比：写作文时，你只能参考之前写的，不允许偷看老师答案。

2. **Cross-Attention（编码器-解码器注意力）**

    * Q 来自 Decoder 自己的输出 Y₁。
    * K、V 来自 Encoder 的输出 X\_encoder。
    * 这样 Decoder 就能“参考输入的句子”。
    * 得到向量 **Y₂**。
      👉 类比：学生在写作文时，不仅看之前写的内容，还能参考“阅读材料”（Encoder 输出）。

3. **Feed Forward Network（前馈神经网络）**

    * 和 Encoder 一样，增强非线性表达能力。
    * 输出向量 **Y₃**。

4. **Residual + Layer Normalization（残差 + 归一化）**

    * 同样的机制，保持稳定。
    * 得到 Decoder 层输出 **Y\_decoder**。

> Decoder 也会堆叠多层，让生成结果越来越精准。

---

## 三、输出层

1. **线性层**：把 Decoder 输出映射到词表大小的向量。
2. **Softmax**：得到每个词作为下一个词的概率。
   👉 类比：最终决定“下一个写的词是谁”，比如概率分布是：苹果 70%，香蕉 20%，小狗 10%。