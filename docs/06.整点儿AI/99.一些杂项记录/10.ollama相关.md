---
title: ollama相关
date: 2024-12-12 16:51:23
permalink: /pages/a3cc11/
categories:
  - 整点儿AI
  - 一些杂项记录
tags:
  - ollama
author:
  name: Mr.Kusch
  link: https://github.com/kuschzzp
---

## ollama相关

[中文文档地址](https://ollama.qianniu.city/)

### 关于安装缓慢

自己改了一下官方脚本上的下载地址，用的我自己的 github 镜像代理地址，注意用量 ！！！！

```shell
curl -fsSL https://img.superkusch.fun/docs/install_ollama.sh | sh
```

### 关于拉取huggingface模型

使用Ollama命令拉取模型，
格式为 `ollama run hf-mirror.com/{username}/{repository}:{quantization}`，其中 `{quantization}` 是模型的量化版本，如果不写的话默认为
`latest`，即最新发布的版本。

例如，拉取一个名为 `Qwen/Qwen2.5-Coder-32B-Instruct` 的模型，命令可以是 `ollama run hf-mirror.com/Qwen/Qwen2.5-Coder-32B-Instruct`。

### 关于下载的模型在哪里

官方常见问题解答地址：[https://github.com/ollama/ollama/blob/main/docs/faq.md#where-are-models-stored](https://github.com/ollama/ollama/blob/main/docs/faq.md#where-are-models-stored)

```
macOS: ~/.ollama/models
Linux: /usr/share/ollama/.ollama/models
Windows: C:\Users\%username%\.ollama\models
```

### 关于配置外网访问

使用`systemctl edit ollama.service`命令编辑systemd服务，将打开一个编辑器。
对每个环境变量，在`[Service]`部分添加一行Environment：:

```
[Service]
Environment="OLLAMA_HOST=0.0.0.0"
```

重新加载

```
systemctl daemon-reload
systemctl restart ollama
```

### 关于多GPU的使用

官方文档：[这里](https://github.com/ollama/ollama/blob/8c9fb8eb73afc220e8bf99772572096b6498b748/docs/gpu.md?plain=1#L36)

```shell
export CUDA_VISIBLE_DEVICES=0,1
```

### 关于 ollama 可视化界面使用

[https://github.com/open-webui/open-webui](https://github.com/open-webui/open-webui)

```  
docker run -d -p 3000:8080 \
--add-host=host.docker.internal:host-gateway \
-v open-webui:/app/backend/data \
--name open-webui --restart always dockerpull.org/dyrnq/open-webui:main
```

